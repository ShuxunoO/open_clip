{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path):\n",
    "    \"\"\"\n",
    "    以只读的方式打开json文件\n",
    "\n",
    "    Args:\n",
    "        config_path: json文件路径\n",
    "\n",
    "    Returns:\n",
    "        A dictionary\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        with open(json_path, 'r', encoding='UTF-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading json file: {}\".format(json_path))\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def save_json(save_path, data):\n",
    "    \"\"\"\n",
    "    Saves the data to a file with the given filename in the given path\n",
    "\n",
    "    Args:\n",
    "        :param save_path: The path to the folder where you want to save the file\n",
    "        :param data: The data to be saved\n",
    "\n",
    "    \"\"\"\n",
    "    with open(save_path, 'w', encoding='UTF-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "class MAKE_NFT1000_DATASET():\n",
    "    def __init__(self, len_per_project, dataset_dict, base_path, out_path) -> None:\n",
    "        self.len_per_project = len_per_project\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.base_path = base_path\n",
    "        self.out_path = out_path\n",
    "\n",
    "    def make_dataset_index(self):\n",
    "        self.make_img_caption_dict()\n",
    "        self.make_img_txt_dict()\n",
    "\n",
    "    def make_img_txt_dict(self):\n",
    "        \"\"\"\n",
    "        制作图片，文本路径字典\n",
    "        \"\"\"\n",
    "        dict_template = {\n",
    "            \"length\" : {},\n",
    "            \"project_name_list\": self.dataset_dict,\n",
    "            \"training_dict\": {},\n",
    "            \"validation_dict\": {},\n",
    "            \"test_dict\": {}\n",
    "            }\n",
    "        # 制作NFT1000的索引文件\n",
    "        for dataset_item in [\"training_list\", \"validation_list\", \"test_list\"]:\n",
    "            project_name_list = self.dataset_dict[dataset_item]\n",
    "            for project_name in tqdm(project_name_list):\n",
    "                # 拼凑字典路径\n",
    "                project_path = self.base_path.joinpath(project_name, \"caption\",  \"_caption_dict.json\")\n",
    "                # 读取字典\n",
    "                caption_dict = load_json(project_path).get(\"caption_dict\")\n",
    "                # 提取字典中的图片名\n",
    "                img_name_list = list(caption_dict.keys())\n",
    "                # 拼凑caption文件路径\n",
    "                img_path_list = [f\"NFT1000/{project_name}/img/{img_name}\" for img_name in list(caption_dict.keys())]\n",
    "                txt_path_list = [f\"NFT1000/{project_name}/caption/{img_name.replace('.png', '.txt')}\" for img_name in img_name_list]\n",
    "                target_caption_dict = {k: v for k, v in zip(img_path_list, txt_path_list)}\n",
    "\n",
    "                if dataset_item == \"training_list\":\n",
    "                    dict_template[\"training_dict\"].update(target_caption_dict)\n",
    "\n",
    "                elif dataset_item == \"validation_list\":\n",
    "                    dict_template[\"validation_dict\"].update(target_caption_dict)\n",
    "\n",
    "                elif dataset_item == \"test_list\":\n",
    "                    dict_template[\"test_dict\"].update(target_caption_dict)\n",
    "\n",
    "        dict_template[\"length\"][\"training_dict\"] = len(dict_template[\"training_dict\"])\n",
    "        dict_template[\"length\"][\"validation_dict\"] = len(dict_template[\"validation_dict\"])\n",
    "        dict_template[\"length\"][\"test_dict\"] = len(dict_template[\"test_dict\"])\n",
    "        # 保存到json文件\n",
    "\n",
    "        print(\"\\n##########  saving……  ##########\\n\")\n",
    "        self.save_json(self.out_path.joinpath(\"_index\", \"NFT1000_img_txt_dict.json\"), dict_template)\n",
    "        print(\"\\n##########  img_txt_dict is saved successfully!  ##########\\n\")\n",
    "\n",
    "    def make_img_caption_dict(self):\n",
    "        dict_template = {\n",
    "            \"length\" : {},\n",
    "            \"project_name_list\": self.dataset_dict,\n",
    "            \"training_dict\": {},\n",
    "            \"validation_dict\": {},\n",
    "            \"test_dict\": {}\n",
    "            }\n",
    "        # 制作NFT1000的索引文件\n",
    "        for dataset_item in [\"training_list\", \"validation_list\", \"test_list\"]:\n",
    "            project_name_list = self.dataset_dict[dataset_item]\n",
    "            for project_name in tqdm(project_name_list):\n",
    "                # 拼凑字典路径\n",
    "                project_path = self.base_path.joinpath(project_name, \"caption\",  \"_caption_dict.json\")\n",
    "                # 读取字典\n",
    "                caption_dict = load_json(project_path).get(\"caption_dict\")\n",
    "                img_path_list = [f\"NFT1000/{project_name}/img/{img_name}\" for img_name in list(caption_dict.keys())]\n",
    "                target_caption_dict = {k: v for k, v in zip(img_path_list, caption_dict.values())}\n",
    "                \n",
    "                if dataset_item == \"training_list\":\n",
    "                    dict_template[\"training_dict\"].update(target_caption_dict)\n",
    "\n",
    "                elif dataset_item == \"validation_list\":\n",
    "                    dict_template[\"validation_dict\"].update(target_caption_dict)\n",
    "\n",
    "                elif dataset_item == \"test_list\":\n",
    "                    dict_template[\"test_dict\"].update(target_caption_dict)\n",
    "\n",
    "        dict_template[\"length\"][\"training_dict\"] = len(dict_template[\"training_dict\"])\n",
    "        dict_template[\"length\"][\"validation_dict\"] = len(dict_template[\"validation_dict\"])\n",
    "        dict_template[\"length\"][\"test_dict\"] = len(dict_template[\"test_dict\"])\n",
    "        # 保存到json文件\n",
    "\n",
    "        print(\"\\n##########  saving……  ##########\\n\")\n",
    "        self.save_json(self.out_path.joinpath(\"_index\", \"NFT1000_img_caption_dict.json\"), dict_template)\n",
    "        print(\"\\n##########  img_caption_dict is saved successfully!  ##########\\n\")\n",
    "\n",
    "    def save_json(self, save_path, data):\n",
    "        \"\"\"\n",
    "        Saves the data to a file with the given filename in the given path\n",
    "\n",
    "        Args:\n",
    "            :param save_path: The path to the folder where you want to save the file\n",
    "            :param data: The data to be saved\n",
    "\n",
    "        \"\"\"\n",
    "        with open(save_path, 'w', encoding='UTF-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load_json(self, json_path):\n",
    "        \"\"\"\n",
    "        以只读的方式打开json文件\n",
    "\n",
    "        Args:\n",
    "            config_path: json文件路径\n",
    "\n",
    "        Returns:\n",
    "            A dictionary\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            with open(json_path, 'r', encoding='UTF-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Error loading json file: {}\".format(json_path))\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "\n",
    "class MAKE_NFT1000_mini_DATASET(MAKE_NFT1000_DATASET):\n",
    "    def __init__(self, len_per_project, dataset_dict, base_path, out_path) -> None:\n",
    "        super().__init__(len_per_project, dataset_dict, base_path, out_path)\n",
    "        self.img_caption_dict = {\n",
    "            \"length\" : {},\n",
    "            \"project_name_list\": self.dataset_dict,\n",
    "            \"training_dict\": {},\n",
    "            \"validation_dict\": {},\n",
    "            \"test_dict\": {}\n",
    "            }\n",
    "        self.img_txt_dict = copy.deepcopy(self.img_caption_dict)\n",
    "\n",
    "    def make_dataset_index(self):\n",
    "        for dataset_item in [\"training_list\", \"validation_list\", \"test_list\"]:\n",
    "            project_name_list = self.dataset_dict[dataset_item]\n",
    "            for project_name in tqdm(project_name_list):\n",
    "                # 拼凑字典路径\n",
    "                project_path = self.base_path.joinpath(project_name, \"caption\",  \"_caption_dict.json\")\n",
    "                # 读取字典\n",
    "                caption_dict = load_json(project_path).get(\"caption_dict\")\n",
    "                \n",
    "                # 对字典进行随机采样\n",
    "                target_caption_dict = self.random_sample(caption_dict)\n",
    "\n",
    "                self.make_img_caption_dict(project_name, target_caption_dict, dataset_item)\n",
    "                self.make_img_txt_dict(project_name, target_caption_dict, dataset_item)\n",
    "        self.save_finally()\n",
    "\n",
    "    def make_img_txt_dict(self, project_name, caption_dict, dataset_item):\n",
    "        img_path_list = [f\"NFT1000/{project_name}/img/{img_name}\" for img_name in caption_dict.keys()]\n",
    "        txt_path_list = [f\"NFT1000/{project_name}/caption/{img_name.replace('.png', '.txt')}\" for img_name in caption_dict.keys()]\n",
    "        target_caption_dict = {k: v for k, v in zip(img_path_list, txt_path_list)}\n",
    "\n",
    "        if dataset_item == \"training_list\":\n",
    "            self.img_txt_dict[\"training_dict\"].update(target_caption_dict)\n",
    "\n",
    "        elif dataset_item == \"validation_list\":\n",
    "            self.img_txt_dict[\"validation_dict\"].update(target_caption_dict)\n",
    "\n",
    "        elif dataset_item == \"test_list\":\n",
    "            self.img_txt_dict[\"test_dict\"].update(target_caption_dict)\n",
    "\n",
    "    def make_img_caption_dict(self, project_name, target_caption_dict, dataset_item):\n",
    "        img_path_list = [f\"NFT1000/{project_name}/img/{img_name}\" for img_name in list(target_caption_dict.keys())]\n",
    "        target_caption_dict = {k: v for k, v in zip(img_path_list, target_caption_dict.values())}\n",
    "                        \n",
    "        if dataset_item == \"training_list\":\n",
    "            self.img_caption_dict[\"training_dict\"].update(target_caption_dict)\n",
    "\n",
    "        elif dataset_item == \"validation_list\":\n",
    "            self.img_caption_dict[\"validation_dict\"].update(target_caption_dict)\n",
    "\n",
    "        elif dataset_item == \"test_list\":\n",
    "            self.img_caption_dict[\"test_dict\"].update(target_caption_dict)\n",
    "            \n",
    "\n",
    "    def random_sample(self, _dict):\n",
    "        if len(_dict) <= self.len_per_project:\n",
    "            return _dict\n",
    "        else:\n",
    "            dict_items_list = list(_dict.items())\n",
    "            sampled_dict = dict(random.sample(dict_items_list, self.len_per_project))\n",
    "            return sampled_dict\n",
    "\n",
    "    def save_finally(self):\n",
    "        self.img_caption_dict[\"length\"][\"training_dict\"] = len(self.img_caption_dict[\"training_dict\"])\n",
    "        self.img_caption_dict[\"length\"][\"validation_dict\"] = len(self.img_caption_dict[\"validation_dict\"])\n",
    "        self.img_caption_dict[\"length\"][\"test_dict\"] = len(self.img_caption_dict[\"test_dict\"])\n",
    "\n",
    "        print(\"\\n##########  saving……  ##########\\n\")\n",
    "        # 保存到json文件\n",
    "        self.save_json(self.out_path.joinpath(\"_index\", \"NFT1000_mini_img_caption_dict.json\"), self.img_caption_dict)\n",
    "        print(\"\\n##########  img_caption_dict is saved successfully!  ##########\\n\")\n",
    "\n",
    "        self.img_txt_dict[\"length\"][\"training_dict\"] = len(self.img_txt_dict[\"training_dict\"])\n",
    "        self.img_txt_dict[\"length\"][\"validation_dict\"] = len(self.img_txt_dict[\"validation_dict\"])\n",
    "        self.img_txt_dict[\"length\"][\"test_dict\"] = len(self.img_txt_dict[\"test_dict\"])\n",
    "\n",
    "        print(\"\\n##########  saving……  ##########\\n\")\n",
    "        self.save_json(self.out_path.joinpath(\"_index\", \"NFT1000_mini_img_txt_dict.json\"), self.img_txt_dict)\n",
    "        print(\"\\n##########  img_txt_dict is saved successfully!  ##########\\n\")\n",
    "\n",
    "\n",
    "class MAKE_NFT100_DATASET(MAKE_NFT1000_mini_DATASET):\n",
    "    def __init__(self, len_per_project, dataset_dict, base_path, out_path) -> None:\n",
    "        super().__init__(len_per_project, dataset_dict, base_path, out_path)\n",
    "\n",
    "    def make_dataset_index(self):\n",
    "        dataset_item = \"training_list\"\n",
    "        project_name_list = random.sample(self.dataset_dict[dataset_item], 80)\n",
    "        for project_name in tqdm(project_name_list):\n",
    "            # 拼凑字典路径\n",
    "            project_path = self.base_path.joinpath(project_name, \"caption\",  \"_caption_dict.json\")\n",
    "            # 读取字典\n",
    "            caption_dict = load_json(project_path).get(\"caption_dict\")\n",
    "            \n",
    "            # 对字典进行随机采样\n",
    "            target_caption_dict = self.random_sample(caption_dict)\n",
    "\n",
    "            self.make_img_caption_dict(project_name, target_caption_dict, dataset_item)\n",
    "            self.make_img_txt_dict(project_name, target_caption_dict, dataset_item)\n",
    "        self.img_caption_dict[\"project_name_list\"].update({\"training_list\": project_name_list})\n",
    "        self.img_txt_dict[\"project_name_list\"].update({\"training_list\": project_name_list})\n",
    "\n",
    "\n",
    "        dataset_item = \"validation_list\"\n",
    "        project_name_list = project_name_list = random.sample(self.dataset_dict[dataset_item], 5)\n",
    "        for project_name in tqdm(project_name_list):\n",
    "            # 拼凑字典路径\n",
    "            project_path = self.base_path.joinpath(project_name, \"caption\",  \"_caption_dict.json\")\n",
    "            # 读取字典\n",
    "            caption_dict = load_json(project_path).get(\"caption_dict\")\n",
    "            \n",
    "            # 对字典进行随机采样\n",
    "            target_caption_dict = self.random_sample(caption_dict)\n",
    "\n",
    "            self.make_img_caption_dict(project_name, target_caption_dict, dataset_item)\n",
    "            self.make_img_txt_dict(project_name, target_caption_dict, dataset_item)\n",
    "        self.img_caption_dict[\"project_name_list\"].update({\"validation_list\": project_name_list})\n",
    "        self.img_txt_dict[\"project_name_list\"].update({\"validation_list\": project_name_list})\n",
    "\n",
    "\n",
    "        dataset_item = \"test_list\"\n",
    "        project_name_list = project_name_list = random.sample(self.dataset_dict[dataset_item], 15)\n",
    "        for project_name in tqdm(project_name_list):\n",
    "            # 拼凑字典路径\n",
    "            project_path = self.base_path.joinpath(project_name, \"caption\",  \"_caption_dict.json\")\n",
    "            # 读取字典\n",
    "            caption_dict = load_json(project_path).get(\"caption_dict\")\n",
    "            \n",
    "            # 对字典进行随机采样\n",
    "            target_caption_dict = self.random_sample(caption_dict)\n",
    "\n",
    "            self.make_img_caption_dict(project_name, target_caption_dict, dataset_item)\n",
    "            self.make_img_txt_dict(project_name, target_caption_dict, dataset_item)\n",
    "        self.img_caption_dict[\"project_name_list\"].update({\"test_list\": project_name_list})\n",
    "        self.img_txt_dict[\"project_name_list\"].update({\"test_list\": project_name_list})\n",
    "        self.save_finally()\n",
    "\n",
    "    def save_finally(self):\n",
    "        self.img_caption_dict[\"length\"][\"training_dict\"] = len(self.img_caption_dict[\"training_dict\"])\n",
    "        self.img_caption_dict[\"length\"][\"validation_dict\"] = len(self.img_caption_dict[\"validation_dict\"])\n",
    "        self.img_caption_dict[\"length\"][\"test_dict\"] = len(self.img_caption_dict[\"test_dict\"])\n",
    "\n",
    "        print(\"\\n##########  saving……  ##########\\n\")\n",
    "        # 保存到json文件\n",
    "        self.save_json(self.out_path.joinpath(\"_index\", \"NFT100_mini_img_caption_dict.json\"), self.img_caption_dict)\n",
    "        print(\"\\n##########  img_caption_dict is saved successfully!  ##########\\n\")\n",
    "\n",
    "        self.img_txt_dict[\"length\"][\"training_dict\"] = len(self.img_txt_dict[\"training_dict\"])\n",
    "        self.img_txt_dict[\"length\"][\"validation_dict\"] = len(self.img_txt_dict[\"validation_dict\"])\n",
    "        self.img_txt_dict[\"length\"][\"test_dict\"] = len(self.img_txt_dict[\"test_dict\"])\n",
    "\n",
    "        print(\"\\n##########  saving……  ##########\\n\")\n",
    "        self.save_json(self.out_path.joinpath(\"_index\", \"NFT100_mini_img_txt_dict.json\"), self.img_txt_dict)\n",
    "        print(\"\\n##########  img_txt_dict is saved successfully!  ##########\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_index = \"/ShuXun_SSD/NFT1000/_index/dataset_index.json\"\n",
    "dataset_index = load_json(dataset_index)\n",
    "print(len(dataset_index[\"training_list\"] + dataset_index[\"validation_list\"] + dataset_index[\"test_list\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = [\"training_dict\", \"validation_dict\", \"test_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 801/801 [00:45<00:00, 17.79it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.78it/s]\n",
      "100%|██████████| 150/150 [00:07<00:00, 19.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########  saving……  ##########\n",
      "\n",
      "\n",
      "##########  img_caption_dict is saved successfully!  ##########\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 801/801 [00:36<00:00, 21.76it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 23.84it/s]\n",
      "100%|██████████| 150/150 [00:06<00:00, 23.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########  saving……  ##########\n",
      "\n",
      "\n",
      "##########  img_txt_dict is saved successfully!  ##########\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 801/801 [00:33<00:00, 23.94it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 25.87it/s]\n",
      "100%|██████████| 150/150 [00:05<00:00, 25.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########  saving……  ##########\n",
      "\n",
      "\n",
      "##########  img_caption_dict is saved successfully!  ##########\n",
      "\n",
      "\n",
      "##########  saving……  ##########\n",
      "\n",
      "\n",
      "##########  img_txt_dict is saved successfully!  ##########\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:03<00:00, 24.54it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 28.06it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 35.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########  saving……  ##########\n",
      "\n",
      "\n",
      "##########  img_caption_dict is saved successfully!  ##########\n",
      "\n",
      "\n",
      "##########  saving……  ##########\n",
      "\n",
      "\n",
      "##########  img_txt_dict is saved successfully!  ##########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    #################################################################################\n",
    "    ############################# 不同终端的替换范围 开始 #############################\n",
    "\n",
    "    source_path = Path(\"/ShuXun_SSD/NFT1000\")\n",
    "    # # 定义要检查的collection排名范围\n",
    "    # task_start = 2\n",
    "    # task_end = 1200\n",
    "    # # 自定义检查范围\n",
    "    # task_list = list(range(task_start, task_end + 1))\n",
    "\n",
    "    # # 1 读取xlsx文件\n",
    "    # wb = openpyxl.load_workbook(str(NFT_1000_excel_path))\n",
    "    # NFT_1000 = wb.get_sheet_by_name(\"NFT1000\")\n",
    "\n",
    "    ############################# 不同终端的替换范围 结束 #############################\n",
    "    #################################################################################\n",
    "\n",
    "\n",
    "    dataset_dict = load_json(source_path.joinpath(\"_index\", \"dataset_index.json\"))\n",
    "    # dataset_dict = fio.load_json(source_path.joinpath(\"_index\", \"dataset_index_copy.json\"))\n",
    "\n",
    "    NFT1000_maker = MAKE_NFT1000_DATASET(10000, dataset_dict, source_path, source_path)\n",
    "    NFT1000_maker.make_dataset_index()\n",
    "\n",
    "    NFT1000_maker = MAKE_NFT1000_mini_DATASET(1000, dataset_dict, source_path, source_path)\n",
    "    NFT1000_maker.make_dataset_index()\n",
    "\n",
    "    NFT1000_maker = MAKE_NFT100_DATASET(500, dataset_dict, source_path, source_path)\n",
    "    NFT1000_maker.make_dataset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFT100_mini_img_caption_dict_path = \"/ShuXun_SSD/NFT1000/_index/NFT100_mini_img_caption_dict.json\"\n",
    "NFT1000_img_caption_dict_path = \"/ShuXun_SSD/NFT1000/_index/NFT1000_img_caption_dict.json\"\n",
    "NFT1000_mini_img_caption_dict_path = \"/ShuXun_SSD/NFT1000/_index/NFT1000_mini_img_caption_dict.json\"\n",
    "base_path = \"/ShuXun_SSD/\"\n",
    "\n",
    "# path_list = [NFT100_mini_img_caption_dict_path, NFT1000_mini_img_caption_dict_path, NFT1000_img_caption_dict_path]\n",
    "path_list = [NFT1000_img_caption_dict_path]\n",
    "task_list = [\"training_dict\", \"validation_dict\", \"test_dict\"]\n",
    "for path_item in path_list:\n",
    "    dict_info = load_json(path_item)\n",
    "    for task_item in task_list:\n",
    "        task_data = dict_info[task_item]\n",
    "        img_path_list = [base_path + img_path for img_path in task_data.keys()]\n",
    "        data_frame = zip(img_path_list, task_data.values())\n",
    "        csv_path = Path(base_path).joinpath(\"NFT1000\", \"_index\", f\"NFT1000_img_caption_dict_path_{task_item}.csv\")\n",
    "        with open(csv_path, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerow([\"filepath\", \"caption\"])\n",
    "            csv_writer.writerows(data_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
